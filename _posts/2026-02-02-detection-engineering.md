---
title: "Detection-as-Code for Wazuh 4.x: Part 0"
tags:
  - Wazuh
  - SIEM
  - Detection
  - Detection Engineering
  - Testing
  - Detection-as-Code
  - Quality Assurance

---

This piece is "Part 0" because it arrived later in time, but earlier in concept. My earlier posts on building a Wazuh development environment and using log replay for behavioral testing grew from practical experimentation rather than from a planned series. Only after those hands-on articles did it become clear that the work pointed toward a broader foundation: whether detection should be treated as configuration or as an engineered system. This article steps back to articulate that philosophical and methodological baseline, and it does so with appreciation for the many practitioners and researchers who have already shaped the field of Detection Engineering and Detection-as-Code long before my own contributions.

## The Professionalization of the SOC: Toward an Engineering Discipline for Detection

The more time I spend working with detection systems, the more I find myself returning to the traditions of engineering as a profession rather than as a metaphor. Engineering has always implied something deeper than construction or configuration. It is a disciplined commitment to using mathematical and natural sciences, refined through study and practice, to design structures and systems that behave predictably under stress. When bridges, aircraft, power grids or industrial machinery are built, they are built against tolerances, boundary conditions and known constraints, rather than intuition alone. That distinction between craft and profession emerged centuries ago, when trial-and-error building gradually gave way to theory-informed design through calculus, thermodynamics and material science. It is the reason physical systems must justify their reliability.

In the digital era, the same transition gave rise to Software Engineering, particularly as a response to the "software crisis" of the 1960s -a period marked by unpredictable behavior, repeated failures and the absence of disciplined processes. The [IEEE 610.12-1990 - IEEE Standard Glossary of Software Engineering Terminology](https://ieeexplore.ieee.org/document/159342) definition captured the pivot: software engineering became the application of a systematic, disciplined and quantifiable approach to the development, operation and maintenance of software. Code ceased to be a personal craft artifact and instead became part of an engineered system whose behavior had to be reasoned about, verified and sustained over time.

The parallel to modern Security Operations Centers is difficult to ignore. Much of what we call detection still operates closer to the earlier craft tradition than to the professional one. Analysts write rules directly in a search interface, tune filters until results "look right," and save them with the hope that the logic will hold when the real conditions arrive later. Success is often measured by whether a rule appears to trigger, not whether it is verifiable, reproducible or resilient under evolving environments. The operational complexity of modern telemetry, especially at enterprise scale, exposes the limits of this approach. Rule drift, data inconsistencies and unintended dependencies accumulate quietly, until the system behaves less like a designed structure and more like an accretion of local fixes.

Detection-as-Code and Detection Engineering emerged partly in response to that reality. My own path into these topics was not planned as a formal series, and neither of these concepts are new -they have existed in various forms for years and have been developed through the efforts of many others in the community. I owe particular appreciation to prior work such as [Kyle Bailey’s Detection Engineering Maturity Matrix](https://kyle-bailey.medium.com/detection-engineering-maturity-matrix-f4f3181a5cc7) and the concise body of thinking aggregated at [detectionengineering.io](https://detectionengineering.io/), both of which frame detection practice as an intentional discipline grounded in lifecycle awareness, operational feedback and engineering rigor. My work sits within that lineage rather than apart from it.

What changed for me was that, through the Wazuh development environment work and through log replay and behavioral validation experiments, I gradually found myself forced to ask whether detection logic should be treated as software instead of configuration. Those earlier articles were practical and exploratory -how to build replay pipelines, how to test rules against controlled input, how to maintain parity across environments. Over time, however, the question widened: if we adopt these practices not as tooling convenience but as professional obligation, what kind of discipline are we actually moving toward?

This article is my attempt to articulate that shift, not as a rigid prescription, but as a reflection on what we can learn from older and more mature engineering traditions. The concepts do not align perfectly. Detection is probabilistic. Telemetry is inconsistent. Adversaries adapt. But the lessons remain meaningful. Where software engineering had decades to formalize its vocabulary -requirements, validation, architecture, lifecycle- detection engineering is only beginning to develop one. The goal is not to force a perfect analogy, but to borrow the parts that make our systems more transparent, testable and responsible.

At the heart of this thinking is the idea that a detection rule is not just a query or pattern filter. It is a software artifact executing a pattern-matching algorithm across a high-velocity data stream. It participates in a larger system that includes decoders, parsers, enrichment logic, storage, scheduling and human response. Once viewed this way, the question is no longer whether rules "work," but whether they behave predictably across time, data variation and operational change. That shift is what begins to move the SOC from craft toward discipline.

A useful lens for this comes from [[ISO/IEC/IEEE 15288 Systems and software engineering — System life cycle processes](https://www.iso.org/standard/81702.html), which frames systems not as isolated components but as interacting elements across a lifecycle. Applied to detection, the pipeline itself becomes the system of interest -from the moment telemetry is generated on an endpoint to the point an alert reaches an analyst. One of the more important refinements that emerges from adopting this framing is recognizing that detections do not simply come into existence and remain indefinitely. In a mature lifecycle, they also evolve, degrade and eventually retire. The 2023 edition of 15288 reinforces retirement as a formal stage, and that maps cleanly to what we often experience informally: old rules that no longer trigger, operate on missing fields or silently produce misleading results, yet are never removed. In an engineered system, retirement is not failure. It is stewardship -an explicit decision to decommission logic when its assumptions no longer hold.

That perspective also clarifies why verification and validation matter. In classical engineering and in [Software Engineering Body of Knowledge (SWEBOK)](https://www.computer.org/education/bodies-of-knowledge/software-engineering), verification asks whether we built the product correctly, while validation asks whether we built the right product. For detection work, this distinction becomes practical rather than philosophical. Regression and replay testing, including the behavioral testing approaches I have explored in Wazuh environments, allow us to validate that logic meaningfully detects the behaviors it claims to detect. At the same time, verification activities ensure that the introduction of a new rule does not break existing ones or distort upstream decision paths. Over time, my own workflow naturally grew into that pattern: first provoke a failure through replay, then refine logic until the detection is both observable and stable, and finally ensure the surrounding evaluation chain continues to behave as expected.

At that point, invariants start to matter. In software engineering, invariants represent conditions that must always remain true for a program or component to function correctly. In detection engineering, the equivalent often lives in the data itself. A detection that assumes a particular field exists for every event type, or that relies on a specific normalization convention, is expressing a form of invariant. When those assumptions are violated, the failure is not merely logical -it becomes systemic. Many of the issues surfaced during replay experiments were not "rule failures," but missing or inconsistent telemetry fields that invalidated the underlying invariant. Viewing these not as local frustrations but as system-level properties helps explain why detection engineering increasingly overlaps with data engineering practice, especially as pipelines incorporate normalization, schema evolution and controlled enrichment.

The architectural lens matters as well. In Wazuh, detections sit within a Directed Acyclic Graph of decoders and rules. That structure implicitly encodes dependencies and evaluation order. When something fails at a lower level -a decoder, a field mapping, a normalization step- the resulting behavior cannot be understood solely by looking at the surface rule. It is a recursive system in the sense described in 15288: each element is itself composed of other elements, and failure propagates through the hierarchy. This reinforces why accidental complexity is such a risk. When rule logic, structural assumptions and data transformations become entangled informally, debugging shifts from engineering problem-solving to archaeology.

One of the unexpected lessons I found through this work was economic rather than technical. Engineering disciplines care deeply about cost because maintainability, reliability and sustainability are inseparable. In detection programs, a significant portion of that cost appears in what [Adrian Sanabria] of [Security Weekly](https://www.youtube.com/@SecurityWeekly) has [described](https://x.com/sawaba/status/1408062070361268231) as the "Customization Tax," the slow and compounding overhead that emerges when every environment, every dataset and every edge case is solved through manual tuning, ad-hoc duplication or isolated fixes. Over time, these modifications become operational debt. They consume analyst time, increase cognitive load and reduce the organization’s ability to improve the system as a whole. Automation, regression suites and consistent rule lifecycles are not abstractions in that sense. They are economic instruments that make the work survivable.

Across the industry, many vendors and platforms have independently converged toward practices that reflect this same direction of travel. Detection-as-Code models treat detection logic as software artifacts that can be versioned, reviewed, tested and deployed through automated workflows. What is interesting in these developments is not the tooling itself, but the engineering instincts they embody. Elastic Security’s emphasis on lifecycle stewardship -from customizable prebuilt rules to alert suppression, historical replay, rule backfill and performance monitoring- treats detections as evolving system elements rather than static filters. Concepts such as building-block rules, where intermediate signals exist to support composition rather than direct triage, implicitly recognize that detections form recursive structures within larger analytical systems. That perspective aligns closely with the lifecycle and quality language found in SWEBOK and 15288, even when it is not described in those terms.

My earlier experiments with Wazuh development environments and log replay sit within the same family of ideas, albeit at a smaller, practitioner-driven scale. What began as a way to understand behavior under controlled conditions evolved into a workflow where replayed sequences exposed gaps in decoders, regressions identified unintended interactions and failures acted as signals rather than surprises. Over time, that shift changed how I thought about detection in general. The important outcome was not that a rule triggered, but that its behavior could be explained, reproduced and sustained.

In that respect, the maturity-oriented framing proposed in the Detection Engineering Maturity Matrix¹ provides a helpful complement to this perspective. Its emphasis on Detection-as-Code practices, responder-centric alert value, and the relationship between detection logic, infrastructure and operational experience resonates strongly with the stewardship and lifecycle themes discussed here. My reflections do not attempt to redefine that work, but rather to place my Wazuh-focused experimentation within the same broader movement toward professionalized, testable and accountable detection systems.

It is worth being explicit about boundaries here, because misunderstanding them risks turning the concept into dogma. Detection Engineering or Detection-as-Code does not mean every team must adopt a specific language, pipeline or repository structure. It does not imply that rules are only legitimate when written in a programming syntax or when every operation mirrors a software delivery lifecycle. The core idea is simply that detections are treated as code in the sense that they are versioned artifacts with intent, assumptions and behavior that can be tested and reasoned about. The form may vary. The discipline does not.

This distinction also prevents the field from drifting into narrow exclusivity. Not every organization has the same constraints, resources or platform flexibility. Engineering practice in detection is not a gatekeeping exercise or a purity test. It is a way of acknowledging that the systems we maintain protect real people and real operations, and therefore deserve the same care we would expect in any other critical infrastructure domain. The more we ground our decisions in verifiable behavior, explicit assumptions and lifecycle awareness, the more sustainable those systems become.

## Boundaries, Constraints and Where This Direction Leads

If there is a recurring theme in both systems engineering and software engineering, it is that constraints are not obstacles to creativity. They define the design space. Detection systems live inside many of them: telemetry availability, data quality, log volume thresholds, human response capacity, regulatory commitments and platform behavior. Treating these as constraints rather than frustrations changes the nature of the work. It moves us from improvisation toward negotiation between desired coverage and feasible operation.

This is also where the connection to data engineering continues to grow. Modern detection workflows increasingly depend on normalization, enrichment and structured event processing pipelines. Whether through message streams, schema-controlled formats or replayable artifacts, the integrity of detection now depends as much on the stability of upstream data as on pattern correctness. Invariants become data invariants. Verification increasingly includes confirming that sources continue to produce the fields that detections require. Validation becomes not only behavioral testing against simulated adversary actions, but also longitudinal testing against representative operational datasets. The closer detection engineering moves toward this intersection, the more it benefits from lessons that data engineering has already internalized.

The convergence we see across practitioners, vendors and open communities suggests that none of this is emerging in isolation. What began as isolated experiments -a development environment here, a replay harness there- now sits alongside industry implementations that formalize similar workflows through CI/CD, rule validation, suppression logic, configuration versioning, historical replay, manual backfill and controlled deployment. The intellectual framing provided by SWEBOK and ISO/IEC/IEEE 15288 offers vocabulary for this shared movement: requirements, architecture, lifecycle, verification, validation, retirement, quality and stewardship. As more teams independently arrive at similar structures and practices, the field begins to develop the social and institutional markers that typically accompany a maturing discipline. In that sense, the creation of the [FIRST Detection Engineering & Threat Hunting Special Interest Group (SIG)](https://www.first.org/global/sigs/de-th/) is not an isolated initiative, but another expression of this same trajectory toward shared definitions, peer collaboration and structured knowledge exchange across global incident response communities.

For those who want to follow the ongoing evolution of this space, the [Detection Engineering Weekly](https://www.detectionengineering.net/) work curated by [Zack Allen](https://www.linkedin.com/in/zack-allen-12749a76/) is an excellent way to observe how the discipline continues to mature in practice, across research, implementations and community reflection. The field is still young, but the direction is increasingly coherent.

## What Detection-as-Code and Detection Engineering Are Not

Before closing, it is useful to clarify what these concepts should not be mistaken for. Detection-as-Code is not a demand that every rule become a software project, nor is it a rejection of analysts who work through graphical tools or search interfaces. It is not defined by a particular syntax, framework or vendor stack. Instead, it is a recognition that detection logic behaves like code even when it does not look like code -it expresses assumptions, depends on upstream structure, produces effects and requires maintenance over time. Treating it as such helps establish ownership, accountability and reproducibility, regardless of whether its representation is YAML, Sigma, Terraform, JSON, XML or a platform’s native query language.

Similarly, Detection Engineering is not an attempt to replace human expertise with process. It does not eliminate judgment, experimentation or adaptive analysis. Rather, it gives those activities a stable foundation by ensuring that what we learn through experience can be translated into structures that endure beyond the moment of discovery. In that sense, it is not an abandonment of craft -it is the continuation of craft within a more reliable system.

## Conclusion

The professionalization of the SOC is not a rhetorical aspiration. It is a gradual, necessary response to complexity, scale and responsibility. By drawing from older engineering traditions -from software to systems and increasingly from data- we gain language and structure that help transform isolated detection rules into coherent, testable and maintainable systems. The goal is not perfection or theoretical purity. It is stewardship: building defenses that behave predictably, evolve responsibly and retire gracefully when their assumptions no longer hold.

Software engineering is not the only discipline we are learning -nor the only one we need to learn from. Data engineering is becoming more deeply integrated into detection processes every day, particularly as telemetry quality, normalization and schema stability shape what detections can reliably achieve. The closer these disciplines continue to converge, the more the SOC begins to resemble what it has long deserved to be: a professional engineering environment rather than an improvisational workshop, grounded not in claims of certainty, but in practices that make our uncertainty measurable, explainable and sustainable.
